{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKLhBAkcqjhuHrh3jN6xs1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### **Fine-tuning Whisper in a Google Colab - Training Pipeline**"],"metadata":{"id":"Ta55qfEpUKHZ"}},{"cell_type":"markdown","source":["### Loading Data\n","\n","We begin by loading the data we saved on our drive in the feature pipeline."],"metadata":{"id":"Nt1p4En0UPlG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6Z9rxBWT6lb"},"outputs":[],"source":["from datasets import load_from_disk\n","common_voice = load_from_disk(\"/content/drive/MyDrive/ML/common_voice\")"]},{"cell_type":"markdown","source":["### Defining a Data Collator"],"metadata":{"id":"JRkhTyGOVJJf"}},{"cell_type":"code","source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"],"metadata":{"id":"i7GO0QEHVE-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"],"metadata":{"id":"yUw7CpqIWdqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation Metric\n","\n","Next, we define the evaluation metric we'll use on our evaluation set. We'll use the Word Error Rate (WER) metric, the 'de-facto' metric for assessing ASR systems."],"metadata":{"id":"CcG2J4JmVfzC"}},{"cell_type":"code","source":["import evaluate\n","\n","metric = evaluate.load(\"wer\")"],"metadata":{"id":"sZ297HmTVeMp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"],"metadata":{"id":"2ezRnjUWVoyq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading Pre-Trained Checkpoint\n","\n","We load the pre-trained Whisper small checkpoint."],"metadata":{"id":"HpT6l4mQVrm3"}},{"cell_type":"code","source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n","model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []\n","model.config.use_cache = False"],"metadata":{"id":"FjKc3XWqVqkA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Defining Training Arguments"],"metadata":{"id":"M7SdShGbWRBf"}},{"cell_type":"code","source":["from transformers import Seq2SeqTrainingArguments\n","from transformers import Seq2SeqTrainer\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/ML/common_voice\",\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=True,\n",")"],"metadata":{"id":"ayUnuz3KWgY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=common_voice[\"train\"],\n","    eval_dataset=common_voice[\"test\"],\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")"],"metadata":{"id":"2IRotk_yWklE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processor.save_pretrained(training_args.output_dir)"],"metadata":{"id":"hvYpRhO4Wzi1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","\n","Here we start training. Since we are using the free Google Colab, our T4 GPU runs only for a limited amount of time, thus we have to collect and save checkpoints to our drive, and continue training, which will be shown in the **Continuing Training** section."],"metadata":{"id":"W1h228rxXBln"}},{"cell_type":"code","source":["from transformers import logging\n","\n","logging.set_verbosity_debug()"],"metadata":{"id":"ryA-Z0DkW-11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"4wEY_5QWXtkH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Continuing Training\n","\n","To continue training, we run all the code above other than the **Training** section. Instead, we run resume training from a selected checkpoint, which loads all the weights, optimizer and scheduler. Once the trainer reaches max steps, we save the model, which saves the best performing checkpoint due to our load_best_model_at_end = True."],"metadata":{"id":"kenRI3ipX_8B"}},{"cell_type":"code","source":["trainer.train(resume_from_checkpoint=\"/content/drive/MyDrive/ML/checkpoints/checkpoint-3000\")"],"metadata":{"id":"HF7Jm3n1YI_n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model()"],"metadata":{"id":"IgOWttOSYq0D"},"execution_count":null,"outputs":[]}]}