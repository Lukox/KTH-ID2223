{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### **Fine-tuning Whisper in a Google Colab - Feature Pipeline**"],"metadata":{"id":"5Ajy3W9mQIYe"}},{"cell_type":"markdown","source":["### Preparing Environment"],"metadata":{"id":"tbHHYqTIQe7X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODvFQhuSEU9L"},"outputs":[],"source":["!pip install datasets>=2.6.1\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install librosa\n","!pip install evaluate>=0.30\n","!pip install jiwer\n","!pip install gradio\n","!pip install accelerate -U"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"vS8JF887GXIJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading Data Set\n","\n","We are using the Polish Common_Voice_11_0 Data Set to fine tune the whisper-small model. Our training data is made up of the train and validation split from common voice, and the test data is its own split."],"metadata":{"id":"_UoTdbIfQ6ed"}},{"cell_type":"code","source":["from datasets import load_dataset, DatasetDict\n","\n","common_voice = DatasetDict()\n","\n","common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"pl\", split=\"train+validation\", token=True)\n","common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"pl\", split=\"test\", token=True)\n","\n","print(common_voice)"],"metadata":{"id":"cx-HhT_TGdWL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We remove the columns that won't be used for our model, as we only require audio file and its transcription/sentence."],"metadata":{"id":"l02ePf9cR_e0"}},{"cell_type":"code","source":["common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n","\n","print(common_voice)"],"metadata":{"id":"ip9NJ_EfG50B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701358815514,"user_tz":-60,"elapsed":391,"user":{"displayName":"David Pham","userId":"16940044462154442854"}},"outputId":"731f066d-4535-4525-d0dd-d51c42f4095e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 24833\n","    })\n","    test: Dataset({\n","        features: ['audio', 'sentence'],\n","        num_rows: 8294\n","    })\n","})\n"]}]},{"cell_type":"markdown","source":["### Acquiring Feature Extractor, Tokenizer and Processor\n","\n","The Whisper model has an associated feature extractor and tokenizer, called WhisperFeatureExtractor and WhisperTokenizer respectively.\n","\n","To simplify using the feature extractor and tokenizer, we can wrap both into a single WhisperProcessor class. This processor object inherits from the WhisperFeatureExtractor and WhisperProcessor and can be used on the audio inputs and model predictions as required. In doing so, we only need to keep track of two objects during training: the processor and the model."],"metadata":{"id":"HNWen-WASLdn"}},{"cell_type":"code","source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"],"metadata":{"id":"nNnsW6CiI1Su"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Polish\", task=\"transcribe\")"],"metadata":{"id":"F6xc437ZJCCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Polish\", task=\"transcribe\")"],"metadata":{"id":"m-OocTDRJups"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sampling Rate\n","\n","The whisper model takes audio at a sampling rate of 16kHz, and the dataset provided by commonvoice has a sampling rate of 4800kHz, thus we sample it down using the Audio import."],"metadata":{"id":"zgJ6qRvVSmJc"}},{"cell_type":"code","source":["print(common_voice[\"train\"][0])"],"metadata":{"id":"t1XfBOc-Jx_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Audio\n","\n","common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"],"metadata":{"id":"SFrBnhjGJ0M1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(common_voice[\"train\"][0])"],"metadata":{"id":"ylD8iQmtJ2um"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prepare Data for Model\n","\n","Now we can write a function to prepare our data ready for the model:\n","\n","We load and resample the audio data by calling batch[\"audio\"]. Datasets performs any necessary resampling operations on the fly.\n","\n","We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n","\n","We encode the transcriptions to label ids through the use of the tokenizer.''\n","\n","Finally, we use the map method to prepare all our data"],"metadata":{"id":"9tZ1aP8RS8Qx"}},{"cell_type":"code","source":["def prepare_dataset(batch):\n","    # load and resample audio data from 48 to 16kHz\n","    audio = batch[\"audio\"]\n","\n","    # compute log-Mel input features from input audio array\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # encode target text to label ids\n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch"],"metadata":{"id":"6AWySH_sJ5Vj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"],"metadata":{"id":"RbQaTV7aJ7dY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Saving Data\n","\n","We save our prepared data on our Google Drive so we can then load it in our training pipeline and train the model"],"metadata":{"id":"VlSN8HHmTWYq"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive',force_remount=True)"],"metadata":{"id":"uAMDi91ILhKd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset, DatasetDict\n","import os\n","\n","output_dir = \"/content/drive/MyDrive/ML/common_voice\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","common_voice.save_to_disk(output_dir)"],"metadata":{"id":"q7Ek07_6Ljc1"},"execution_count":null,"outputs":[]}]}